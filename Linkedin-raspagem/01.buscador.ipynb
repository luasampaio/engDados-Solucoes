{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e82c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c89510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Set\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_job_ids(\n",
    "    title: str,\n",
    "    location: str,\n",
    "    num_pages: int,\n",
    "    pause: float = 0.5,\n",
    "    verbose: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Busca job_ids de vagas no LinkedIn Jobs (endpoint público 'seeMoreJobPostings').\n",
    "\n",
    "    :param title: termo de busca (ex: 'engineer', 'scientist').\n",
    "    :param location: localização (ex: 'Brazil', 'Lisbon, Portugal').\n",
    "    :param num_pages: número de páginas a percorrer.\n",
    "    :param pause: tempo de espera entre requisições (segundos).\n",
    "    :param verbose: se True, imprime logs simples.\n",
    "    :return: lista de job_ids (sem duplicados).\n",
    "    \"\"\"\n",
    "    job_ids: Set[str] = set()\n",
    "\n",
    "    # Prepara parâmetros com encoding seguro para URL\n",
    "    keywords = f\"data {title}\".strip()\n",
    "    keywords_encoded = quote_plus(keywords)\n",
    "    location_encoded = quote_plus(location)\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(HEADERS)\n",
    "\n",
    "        for page in range(num_pages):\n",
    "            start = page * 10  # cada página costuma ter 10 vagas\n",
    "            list_url = (\n",
    "                \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "                f\"?keywords={keywords_encoded}\"\n",
    "                f\"&location={location_encoded}\"\n",
    "                \"&position=1&pageNum=0\"\n",
    "                f\"&start={start}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = session.get(list_url, timeout=10)\n",
    "            except requests.RequestException as e:\n",
    "                if verbose:\n",
    "                    print(f\"[ERRO] Falha na requisição da página {page}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"[INFO] Página {page} - status {response.status_code}\")\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                # 429 = too many requests, 404, etc.\n",
    "                if verbose:\n",
    "                    print(f\"[AVISO] Pulando página {page} (status {response.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            page_jobs = soup.find_all(\"li\")\n",
    "\n",
    "            if not page_jobs and verbose:\n",
    "                print(f\"[AVISO] Nenhuma vaga encontrada na página {page}\")\n",
    "\n",
    "            for job in page_jobs:\n",
    "                base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "                if not base_card_div:\n",
    "                    continue\n",
    "\n",
    "                urn = base_card_div.get(\"data-entity-urn\")\n",
    "                if not urn:\n",
    "                    continue\n",
    "\n",
    "                parts = urn.split(\":\")\n",
    "                if len(parts) < 4:\n",
    "                    continue\n",
    "\n",
    "                job_id = parts[3]\n",
    "                job_ids.add(job_id)\n",
    "                if verbose:\n",
    "                    print(f\"[JOB ID] {job_id}\")\n",
    "\n",
    "            # Pequena pausa para evitar 429\n",
    "            time.sleep(pause)\n",
    "\n",
    "    return list(job_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e96b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def get_text_or_none(elem) -> Optional[str]:\n",
    "    \"\"\"Retorna o texto stripado ou None se elem for None.\"\"\"\n",
    "    return elem.get_text(strip=True) if elem else None\n",
    "\n",
    "\n",
    "def safe_criteria(criteria_list, index: int) -> Optional[str]:\n",
    "    \"\"\"Retorna o critério se existir; caso contrário, None.\"\"\"\n",
    "    try:\n",
    "        return criteria_list[index].get_text(strip=True)\n",
    "    except (IndexError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_job_page(html: str, job_id: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Extrai os dados de uma página de vaga do LinkedIn.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    job_post: Dict[str, Optional[str]] = {}\n",
    "\n",
    "    # Título da vaga\n",
    "    job_post[\"job_title\"] = get_text_or_none(\n",
    "        soup.find(\n",
    "            \"h2\",\n",
    "            {\n",
    "                \"class\": (\n",
    "                    \"top-card-layout__title font-sans text-lg \"\n",
    "                    \"papabear:text-xl font-bold leading-open \"\n",
    "                    \"text-color-text mb-0 topcard__title\"\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Nome da empresa\n",
    "    job_post[\"company_name\"] = get_text_or_none(\n",
    "        soup.find(\n",
    "            \"a\",\n",
    "            {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Critérios (nível de experiência, tipo de contrato, etc.)\n",
    "    criteria = soup.find_all(\n",
    "        \"span\",\n",
    "        {\n",
    "            \"class\": (\n",
    "                \"description__job-criteria-text \"\n",
    "                \"description__job-criteria-text--criteria\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    job_post[\"experience_level\"] = safe_criteria(criteria, 0)\n",
    "    job_post[\"type_of_contract\"] = safe_criteria(criteria, 1)\n",
    "    # Se quiser voltar com os outros, é só descomentar:\n",
    "    # job_post[\"job_role\"] = safe_criteria(criteria, 2)\n",
    "    # job_post[\"field\"] = safe_criteria(criteria, 3)\n",
    "\n",
    "    # Easy apply (True/False)\n",
    "    try:\n",
    "        offsite_icon = soup.find(\n",
    "            \"icon\",\n",
    "            {\"data-svg-class-name\": \"apply-button__offsite-apply-icon-svg\"},\n",
    "        )\n",
    "        # Se tem ícone de offsite, não é easy apply\n",
    "        job_post[\"easy_apply\"] = False if offsite_icon else True\n",
    "    except Exception:\n",
    "        job_post[\"easy_apply\"] = None\n",
    "\n",
    "    # Tempo desde a postagem\n",
    "    job_post[\"time_posted\"] = get_text_or_none(\n",
    "        soup.find(\n",
    "            \"span\",\n",
    "            {\n",
    "                \"class\": (\n",
    "                    \"posted-time-ago__text \"\n",
    "                    \"topcard__flavor--metadata\"\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Número de candidatos\n",
    "    job_post[\"num_applicants\"] = get_text_or_none(\n",
    "        soup.find(\"span\", {\"class\": \"num-applicants__caption\"})\n",
    "    )\n",
    "\n",
    "    # Link da vaga\n",
    "    job_post[\"job_link\"] = f\"https://www.linkedin.com/jobs/view/{job_id}/\"\n",
    "\n",
    "    return job_post\n",
    "\n",
    "\n",
    "def scrape_jobs(id_list: List[str], sleep_seconds: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"Faz scrape de uma lista de job_ids do LinkedIn.\"\"\"\n",
    "    job_list: List[Dict] = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(HEADERS)\n",
    "\n",
    "        for job_id in id_list:\n",
    "            job_url = (\n",
    "                f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = session.get(job_url, timeout=10)\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Erro ao requisitar {job_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            print(job_id, response.status_code)\n",
    "\n",
    "            # 200 = ok; 429 = many requests; 404 = não encontrado etc.\n",
    "            if response.status_code != 200:\n",
    "                # aqui você pode decidir logar / armazenar o erro\n",
    "                continue\n",
    "\n",
    "            job_post = parse_job_page(response.text, job_id)\n",
    "            job_list.append(job_post)\n",
    "\n",
    "            # Uma pequena pausa ajuda a evitar 429\n",
    "            time.sleep(sleep_seconds)\n",
    "\n",
    "    return job_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7a31ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Página 0 - status 200\n",
      "[JOB ID] 4318327757\n",
      "[JOB ID] 4308497997\n",
      "[JOB ID] 4281828542\n",
      "[JOB ID] 4233957493\n",
      "[JOB ID] 4337472760\n",
      "[JOB ID] 4312919215\n",
      "[JOB ID] 4298239221\n",
      "[JOB ID] 4311720804\n",
      "[JOB ID] 4324753186\n",
      "[JOB ID] 4310476364\n",
      "[INFO] Página 1 - status 200\n",
      "[JOB ID] 4317491191\n",
      "[JOB ID] 4303420983\n",
      "[JOB ID] 4231033954\n",
      "[JOB ID] 4312301348\n",
      "[JOB ID] 4316823422\n",
      "[JOB ID] 4331368566\n",
      "[JOB ID] 4314223388\n",
      "[JOB ID] 3969555543\n",
      "[JOB ID] 4334041808\n",
      "[JOB ID] 4315447335\n",
      "[INFO] Página 2 - status 200\n",
      "[JOB ID] 4331348122\n",
      "[JOB ID] 4334386540\n",
      "[JOB ID] 4307252914\n",
      "[JOB ID] 4245979421\n",
      "[JOB ID] 4333019146\n",
      "[JOB ID] 4320660148\n",
      "[JOB ID] 4307685458\n",
      "[JOB ID] 4214699583\n",
      "[JOB ID] 4295464589\n",
      "[JOB ID] 4313394952\n",
      "[INFO] Página 3 - status 200\n",
      "[JOB ID] 4312861310\n",
      "[JOB ID] 4334181171\n",
      "[JOB ID] 4318515516\n",
      "[JOB ID] 4320613907\n",
      "[JOB ID] 4091160916\n",
      "[JOB ID] 4337456861\n",
      "[JOB ID] 4333210689\n",
      "[JOB ID] 4304754197\n",
      "[JOB ID] 4318523103\n",
      "[JOB ID] 4305932314\n",
      "[INFO] Página 4 - status 200\n",
      "[JOB ID] 4307544126\n",
      "[JOB ID] 4320251804\n",
      "[JOB ID] 4304781155\n",
      "[JOB ID] 4334206551\n",
      "[JOB ID] 4321299024\n",
      "[JOB ID] 4331799171\n",
      "[JOB ID] 4321094496\n",
      "[JOB ID] 4333139844\n",
      "[JOB ID] 4332173545\n",
      "[JOB ID] 4021367241\n"
     ]
    }
   ],
   "source": [
    "title = \"engineer\"\n",
    "location = \"Brazil\"\n",
    "num_pages = 5\n",
    "\n",
    "id_list = fetch_job_ids(title, location, num_pages)\n",
    "##print(\"Total de IDs coletados:\", len(id_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d612e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234567890 404\n",
      "0987654321 404\n"
     ]
    }
   ],
   "source": [
    "id_list = [\"1234567890\", \"0987654321\"]\n",
    "jobs = scrape_jobs(id_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
