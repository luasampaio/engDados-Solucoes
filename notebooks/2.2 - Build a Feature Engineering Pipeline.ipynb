{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e088ba2-b938-4173-8f19-75caee37c8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ab248b-58d1-4c16-a60b-2a3209064a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Construir um Pipeline de Engenharia de Recursos\n",
    "\n",
    "Nesta demonstração, construiremos um pipeline de engenharia de recursos para gerenciar o carregamento de dados, imputação e transformação. O pipeline será aplicado nos conjuntos de treinamento, teste e validação, com os resultados exibidos. A etapa final envolve salvar o pipeline em disco para uso futuro, garantindo uma preparação de dados eficiente e consistente para tarefas de aprendizado de máquina.\n",
    "\n",
    "**Objetivos de Aprendizagem:**\n",
    "\n",
    "*Ao final desta demonstração, você será capaz de:*\n",
    "\n",
    "* Criar um pipeline de preparação de dados e engenharia de recursos com múltiplas etapas.\n",
    "* Criar um pipeline com tarefas para imputação e transformação de dados.\n",
    "* Aplicar um pipeline de preparação de dados e engenharia de recursos em um conjunto de treinamento/modelagem e um conjunto de retenção.\n",
    "* Exibir os resultados da transformação.\n",
    "* Salvar um pipeline de preparação de dados e engenharia de recursos para uso futuro potencial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d34f434-d69d-4e39-9bc5-5414ffe4d788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requisitos\n",
    "\n",
    "Por favor, revise os seguintes requisitos antes de iniciar a lição:\n",
    "\n",
    "* Para executar este notebook, você precisa usar uma das seguintes versões do Databricks: **13.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d389410-29ef-4031-a7cf-f5bd3d791ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Configuração da Sala de Aula\n",
    "\n",
    "Antes de iniciar a demonstração, execute o script de configuração da sala de aula fornecido. Este script definirá as variáveis de configuração necessárias para a demonstração. Execute a célula a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2778af34-951c-477f-a4d0-174f76c3c2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c97d0b-d2fe-445f-97f3-fd25a076144f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Outras Convenções:**\n",
    "\n",
    "Durante esta demonstração, vamos nos referir ao objeto `DA`. Este objeto, fornecido pela Databricks Academy, contém **variáveis como seu nome de usuário, nome do catálogo, nome do esquema, diretório de trabalho e locais do conjunto de dados**. Execute o bloco de código abaixo para visualizar esses detalhes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6f0fbc1-4eaa-4c3f-b1a7-ddf398a97dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3ba56b-ab2a-41b5-8653-6ac27bdb13b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Preparação de Dados\n",
    "\n",
    "Antes de construir o pipeline, garantiremos a consistência no conjunto de dados convertendo colunas Inteiro e Booleano para o tipo de dados Double e lidando com valores ausentes em colunas numéricas e de string dentro do conjunto de dados **`Telco`**. Estes são os passos que seguiremos nesta seção.\n",
    "\n",
    "1. Carregar conjunto de dados\n",
    "\n",
    "1. Dividir conjunto de dados em conjuntos de treino e teste\n",
    "\n",
    "1. Convertendo Colunas Inteiro e Booleano para Double\n",
    "\n",
    "1. Lidando com Valores Ausentes\n",
    "\n",
    "  * Colunas Numéricas\n",
    "\n",
    "  * Colunas de String\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bfdc3d-34ad-4f80-b981-c7bac439f5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Carregar Conjunto de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77921ae1-e6f8-4a43-9076-cbf46ef2379d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Carregar conjunto de dados\n",
    "dataset_path = f\"{DA.paths.datasets}/telco/telco-customer-churn-missing.csv\"\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# Selecionar colunas de interesse\n",
    "telco_df = telco_df.select(\"gender\", \"SeniorCitizen\", \"Partner\", \"tenure\", \"InternetService\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"TotalCharges\", \"Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869593ae-9d0a-4822-98c0-d48c4b096102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pré-processamento rápido\n",
    "* `SeniorCitizen` como `boolean`\n",
    "* `TotalCharges` como `double`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84532f82-ade6-4bd4-94ad-3248b84f528d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# substitua valores \"null\" com Null\n",
    "# para coluna em telco_df.colunas:\n",
    "# telco_df = telco_df.withColumn(coluna, quando(col(coluna) == \"null\", None).caso contrário(col(coluna)))\n",
    "\n",
    "# limpar colunas\n",
    "telco_df = telco_df.withColumn(\"SeniorCitizen\", when(col(\"SeniorCitizen\")==1, True).otherwise(False))\n",
    "telco_df = telco_df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
    "\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b4f7f4b-304c-4014-9a92-cd54d122d367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Divisão Treino / Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edfca735-b66d-4247-bf94-d4a4c5c33d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a92cea2-c3d1-4808-879d-314d6e807b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformar Conjunto de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd17fe1a-41b3-4ece-a904-c6107cc8dd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "\n",
    "# Obtenha uma lista de colunas inteiras e booleanas\n",
    "integer_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "# Faça um loop pelas colunas inteiras para converter cada uma em dupla\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == StringType()]\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "\n",
    "# Obtenha uma lista de colunas com valores ausentes\n",
    "# Numérico\n",
    "num_missing_values_logic = [count(when(col(column).isNull(),column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().asDict()\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "# String\n",
    "string_missing_values_logic = [count(when(col(column).isNull(),column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"Colunas numéricas com valores ausentes: {num_missing_cols}\")\n",
    "print(f\"Colunas de string com valores ausentes: {string_missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21c5a899-75e4-4e33-9545-e8ddc0d2b60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Criar um Pipeline\n",
    "\n",
    "Define um pipeline Spark ML para pré-processar um conjunto de dados, incluindo a indexação de colunas categóricas, a imputação de valores ausentes, o dimensionamento de recursos numéricos, a codificação one-hot em recursos categóricos e a montagem do vetor de recurso final para aprendizado de máquina.\n",
    "\n",
    "Neste pipeline Spark ML, pré-processamos um conjunto de dados para prever a rotatividade de clientes em uma empresa de telecomunicações **`telco`**. O pipeline inclui as seguintes etapas-chave:\n",
    "\n",
    "* **Converter Colunas Categóricas em Índices Numéricos:**\n",
    "Esta etapa converte colunas categóricas em índices numéricos, permitindo que o modelo processe dados categóricos.\n",
    "\n",
    "* **Imputar Valores Ausentes:**\n",
    "O Imputer é usado para preencher valores ausentes em **colunas numéricas com valores ausentes (por exemplo, `tenure`, `TotalCharges`) usando a estratégia `mean`**, garantindo que o conjunto de dados esteja completo e pronto para análise.\n",
    "**Valores categóricos ausentes serão automaticamente codificados como uma categoria separada.**\n",
    "\n",
    "* **VectorAssembler e RobustScaler:**\n",
    "Essas etapas combinam colunas numéricas relevantes em um vetor de recurso e, em seguida, dimensionam os recursos para reduzir a sensibilidade a outliers.\n",
    "\n",
    "* **Executar Codificação One Hot em Variável Categórica:**\n",
    "Esta etapa converte as colunas categóricas indexadas em vetores esparsos binários, permitindo que o modelo processe dados categóricos de forma eficaz.\n",
    "\n",
    "* **Pipeline:**\n",
    "Todas essas etapas são encapsuladas em um Pipeline, fornecendo uma maneira conveniente e reproduzível de pré-processar os dados para tarefas de aprendizado de máquina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e84e4b-b457-4106-96f8-9310cf2f0687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, RobustScaler, StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "# Imputer (estratégia de média para todos os double/numéricos)\n",
    "to_impute = num_missing_cols\n",
    "imputer = Imputer(inputCols=to_impute, outputCols=to_impute, strategy='mode')\n",
    "\n",
    "# Escala numérica\n",
    "numerical_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"numerical_assembled\")\n",
    "numerical_scaler = RobustScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "\n",
    "# String/Cat Indexer (irá codificar missing/null como índice separado)\n",
    "string_cols_indexed = [c + '_index' for c in string_cols]\n",
    "string_indexer = StringIndexer(inputCols=string_cols, outputCols=string_cols_indexed, handleInvalid=\"keep\")\n",
    "\n",
    "# OHE categóricas\n",
    "ohe_cols = [column + '_ohe' for column in string_cols]\n",
    "one_hot_encoder = OneHotEncoder(inputCols=string_cols_indexed, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "# Assembler (Todos)\n",
    "feature_cols = [\"numerical_scaled\"] + ohe_cols\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Instanciar o pipeline\n",
    "stages_list = [\n",
    "    imputer,\n",
    "    numerical_assembler,\n",
    "    numerical_scaler,\n",
    "    string_indexer,\n",
    "    one_hot_encoder,\n",
    "    vector_assembler\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=stages_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c51c22d-4e73-4f79-8673-43763b5b6588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ajustar o Pipeline\n",
    "\n",
    "No contexto de aprendizado de máquina e MLflow, **`fitting`** corresponde ao processo de treinamento de um modelo de aprendizado de máquina em um conjunto de dados especificado.\n",
    "\n",
    "Na etapa anterior, criamos um pipeline. Agora, ajustaremos um modelo com base no pipeline. Este pipeline indexará colunas de string, imputará colunas especificadas, dimensionará colunas numéricas, codificará especificamente colunas em um-hot, e finalmente criará um vetor de todas as colunas de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e195645-ed92-46da-a321-b402f581f0bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26a15fd-fa44-4b82-95e3-5e9b7a6bbe9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Em seguida, podemos usar este modelo para transformar, ou aplicar, em qualquer conjunto de dados que desejarmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a92c17-a9e4-496f-ba3f-188fd94c1539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transforme training_df e test_df\n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e35414c-d5ec-4d4a-9043-b6720210dc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_transformed_df.select(\"features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7a86d3-6c4e-40f9-8ada-b856f0b94bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvar e Reutilizar o Pipeline\n",
    "\n",
    "Preservar o pipeline de previsão de rotatividade de clientes de telecomunicações, que abrange o modelo, parâmetros e metadados, é fundamental para manter a reprodutibilidade, permitir o controle de versão e facilitar a colaboração entre os membros da equipe. Isso garante um registro detalhado do fluxo de trabalho de aprendizado de máquina. Nesta seção, seguiremos estas etapas;\n",
    "\n",
    "1. **Salvar o Pipeline:** Salve o modelo de pipeline, incluindo todos os componentes relevantes, no armazenamento de artefatos designado. O pipeline salvo é organizado dentro da pasta **`spark_pipelines`** para maior clareza.\n",
    "\n",
    "1. **Explorar Estágios do Pipeline Carregado:** Ao carregar o pipeline, inspecione os estágios para revelar as principais transformações e compreender a sequência de operações aplicadas durante a execução do pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b414789c-e69c-470a-99c4-13d060b4e7f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Salvar o Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf217033-e048-461f-a033-914e469315fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model.save(f\"{DA.paths.working_dir}/spark_pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ab49d7-2f40-47f2-b354-3c34b8be2767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Carregar e Usar Modelo Salvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7c7998-7b67-48d3-a357-474220bb7cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "# Carregar o pipeline\n",
    "loaded_pipeline = PipelineModel.load(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "\n",
    "# Mostrar etapas do pipeline\n",
    "loaded_pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5beee621-469d-41e2-8878-3b6d75780770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vamos usar o pipeline carregado para transformar o conjunto de dados de teste\n",
    "test_transformed_df = loaded_pipeline.transform(test_df)\n",
    "display(test_transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ebfa6f-bec0-4243-b689-162cc8315df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Limpar Sala de Aula\n",
    "\n",
    "Execute a célula a seguir para remover os ativos específicos de aula criados durante esta lição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c4b4c6-82fe-4d75-ba47-793a9549e18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d07558c-d878-48c0-8216-4007bab8aca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusão\n",
    "\n",
    "Em resumo, o pipeline de engenharia de recursos apresentado nesta demonstração oferece uma abordagem sistemática e consistente para lidar com o carregamento de dados, imputação e transformação. Ao demonstrar sua aplicação em diferentes conjuntos e enfatizar a importância da preparação de dados, o pipeline se prova uma ferramenta valiosa para tarefas de aprendizado de máquina eficientes e reproduzíveis.\n",
    "\n",
    "A etapa final de salvar o pipeline no disco garante usabilidade futura, aumentando a eficácia geral do processo de preparação de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5eab8c-f2fc-4b1e-a803-d82b9b850223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.2 - Build a Feature Engineering Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
