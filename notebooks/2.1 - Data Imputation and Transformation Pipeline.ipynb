{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8de4077-8ebc-4ca1-a691-01b1653dd618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675aa350-f4a4-48c2-8e70-2ece515f3a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Preparação de Dados e Engenharia de Recursos\n",
    "\n",
    "Nesta demonstração, vamos mergulhar em técnicas como preparar dados de modelagem, incluindo dividir dados, lidar com valores ausentes, codificar recursos categóricos e padronizar recursos. Também discutiremos a remoção de outliers e a coerção de colunas para o tipo de dados correto. Ao final, você terá uma compreensão abrangente da preparação de dados para modelagem e preparação de recursos.\n",
    "\n",
    "**Objetivos de Aprendizagem:**\n",
    "\n",
    "Ao final desta demonstração, você será capaz de:\n",
    "\n",
    "- Coagir colunas para serem do tipo de dados correto com base no tipo de recurso ou variável de destino.\n",
    "- Identificar e remover outliers dos dados de modelagem.\n",
    "- Descartar linhas/colunas que contenham valores ausentes.\n",
    "- Imputar valores ausentes categóricos com o valor de moda.\n",
    "- Substituir valores ausentes por um valor de substituição especificado.\n",
    "- Codificar recursos categóricos com um-hot.\n",
    "- Realizar indexação ordenada como uma alternativa de preparação de recursos categóricos para modelagem de floresta aleatória.\n",
    "- Aplicar incorporações pré-existentes a recursos categóricos.\n",
    "- Padronizar recursos em um conjunto de treinamento.\n",
    "- Dividir dados de modelagem em um dividido treino-teste-retenção como parte de um processo de modelagem.\n",
    "- Dividir dados de treinamento em conjuntos de validação cruzada como parte de um processo de modelagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d467109-7340-4767-a106-5804e6f089d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requisitos\n",
    "\n",
    "Por favor, revise os seguintes requisitos antes de iniciar a lição:\n",
    "\n",
    "* Para executar este notebook, você precisa usar um dos seguintes tempos de execução do Databricks: **13.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff3a391-20a9-43ff-917f-37ef018fd99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Configuração da Sala de Aula\n",
    "\n",
    "Antes de iniciar a demonstração, execute o script de configuração da sala de aula fornecido. Este script definirá as variáveis de configuração necessárias para a demonstração. Execute a célula a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57890707-2eb9-448f-962f-5a50c44683b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d81b985-dff6-4651-be84-6e83897cad7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Outras Convenções:**\n",
    "\n",
    "Durante esta demonstração, vamos nos referir ao objeto `DA`. Este objeto, fornecido pela Databricks Academy, contém variáveis como seu nome de usuário, nome do catálogo, nome do esquema, diretório de trabalho e locais dos conjuntos de dados. Execute o bloco de código abaixo para visualizar esses detalhes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c747e88d-8b60-4659-8de3-f8e1888d5d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f884b6ed-48cc-4db0-b525-186116443c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Limpeza e Imputação de Dados\n",
    "\n",
    "- Carregue o conjunto de dados do caminho especificado usando Spark e leia-o como um DataFrame.\n",
    "\n",
    "- Remova quaisquer linhas com valores ausentes do DataFrame usando o método **`dropna()`**.\n",
    "\n",
    "- Preencha quaisquer valores ausentes restantes no DataFrame com 0 usando o método **`fillna()`**.\n",
    "\n",
    "- Crie uma visualização temporária chamada **`telco_customer_churn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0799ee25-e01f-47b7-8a4e-b1505b2189f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = f\"{DA.paths.datasets}/telco/telco-customer-churn-noisy.csv\"\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# telco_df.printSchema()\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a90b2f6-0e61-40dd-8ee6-909f4a23c763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Coagir/Corrigir Tipos de Dados\n",
    "\n",
    "Mesmo que a maioria dos tipos de dados esteja correta, vamos fazer o seguinte para ter uma pegada de memória melhor do dataframe na memória\n",
    "\n",
    "* Converter as colunas binárias **`SeniorCitizen`** e **`Churn`** para o tipo booleano.\n",
    "\n",
    "* Convertendo a coluna **`tenure`** para um inteiro longo usando **`.selectExpr`** e reordenando as colunas.\n",
    "\n",
    "* Usando **`spark.sql`** para converter as colunas **`Partner`**, **`Dependents`**, **`PhoneService`** e **`PaperlessBilling`** para o tipo booleano, e reordenando as colunas. Em seguida, salvando o dataframe como uma tabela DELTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa09170-d3a7-4976-a5ec-88981b416f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, ShortType, IntegerType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "binary_columns = [\"SeniorCitizen\", \"Churn\"]\n",
    "telco_customer_churn_df = telco_df\n",
    "for column in binary_columns:\n",
    "    telco_customer_churn_df = telco_df.withColumn(column, col(column).cast(BooleanType()))\n",
    "\n",
    "telco_customer_churn_df.select(*binary_columns).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f01906bc-2a78-420b-aace-4d4d818a29ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A conversão não funcionou em `SeniorCitizen` provavelmente porque havia alguns valores nulos ou valores que não puderam ser codificados corretamente, podemos forçar a coerção usando um método de filtro simples (supondo que valores ausentes nesta coluna possam ser codificados como `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679cbeda-6514-4a22-982c-037667bc6bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_churn_df = telco_customer_churn_df.withColumn(\\\n",
    "    \"SeniorCitizen\", when(col(\"SeniorCitizen\")==1, True).otherwise(False))\n",
    "\n",
    "telco_customer_churn_df.select(\"SeniorCitizen\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7966d78-c22a-4abb-8e6e-8a69889534f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PhoneService & PaperlessBilling para novo booleano usando spark.sql e reordenando colunas\n",
    "telco_customer_churn_df.createOrReplaceTempView(\"telco_customer_churn_temp_view\")\n",
    "\n",
    "telco_customer_casted_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        customerID,\n",
    "        BOOLEAN(Dependents),\n",
    "        BOOLEAN(Partner),\n",
    "        BOOLEAN(PhoneService),\n",
    "        BOOLEAN(PaperlessBilling),\n",
    "        * \n",
    "        EXCEPT (customerID, Dependents, Partner, PhoneService, PaperlessBilling, Churn),\n",
    "        Churn\n",
    "    FROM telco_customer_churn_temp_view\n",
    "\"\"\")\n",
    "\n",
    "telco_customer_casted_df.select(\"Dependents\",\"Partner\",\"PaperlessBilling\", \"PhoneService\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8dbac0b-fb87-4dfd-a525-33e4a760671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mês de permanência para Long/Integer usando.selectExpr\n",
    "telco_customer_casted_df = telco_customer_churn_df.selectExpr(\"* except(tenure)\", \"cast(tenure as long) tenure\")\n",
    "telco_customer_casted_df.select(\"tenure\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b402ceeb-5952-4ea6-a882-e8e7ac16f709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_name_full = \"telco_customer_full\"\n",
    "\n",
    "# [OPCIONAL] Salvar como tabela DELTA (silver)\n",
    "telco_customer_full_silver = f\"{telco_customer_name_full}_silver\"\n",
    "telco_customer_casted_df.write.mode(\"overwrite\").option(\"mergeSchema\",True).saveAsTable(telco_customer_full_silver)\n",
    "# print(telco_customer_casted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc522d76-758f-45c1-92e3-f5996d827333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Tratamento de Outliers\n",
    "\n",
    "Veremos como lidar com outliers em uma coluna, identificando e abordando pontos de dados que se encontram muito fora do intervalo típico de valores em um conjunto de dados. Métodos comuns para lidar com outliers incluem removê-los, filtrá-los, transformar os dados ou substituir outliers por valores mais representativos.\n",
    "\n",
    "Siga estas etapas para lidar com outliers:\n",
    "* Crie uma nova tabela silver chamada **`telco_customer_full_silver`** acrescentando **`silver`** ao nome da tabela original e, em seguida, acesse-a usando Spark SQL.\n",
    "\n",
    "* Filtre os outliers da coluna **`TotalCharges`** removendo as linhas em que o valor da coluna excede o valor limite especificado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f447e6-794b-4716-9f97-c0d012971928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Removendo sujos da coluna **`TotalCharges`** removendo linhas onde o valor da coluna excede o valor de corte especificado (por exemplo, valores negativos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e30c3a9-dbcc-4e1a-859b-1e80874c44bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_casted_df.select(\"TotalCharges\", \"tenure\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ceee0e7-164d-4d75-baa9-2a53fe9758d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Remover clientes com TotalCharges negativos\n",
    "TotalCharges_cutoff = 0\n",
    "\n",
    "# Use o método.filter e a função SQL col()\n",
    "telco_no_outliers_df = telco_customer_casted_df.filter(\\\n",
    "    (col(\"TotalCharges\") > TotalCharges_cutoff) | \\\n",
    "    (col(\"TotalCharges\").isNull())) # Manter Nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fafb90b-fca5-423a-a8c5-72bffc67a62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Removendo outliers do PaymentMethod**\n",
    "* Identifique os dois grupos com menor ocorrência na coluna **`PaymentMethod`** e calcule a contagem total e a média de **`MonthlyCharges`** para cada grupo.\n",
    "\n",
    "* Remova os clientes dos grupos identificados de baixa ocorrência na coluna **`PaymentMethod`** para filtrar os outliers.\n",
    "\n",
    "* Crie um novo dataframe **`telco_filtered_df`** contendo os dados filtrados.\n",
    "\n",
    "* Compare a contagem de registros antes e depois dividindo a contagem de **`telco_casted_full_df`** e **`telco_no_outliers_df`** dataframe removendo outliers e, em seguida, materialize o dataframe resultante como uma nova tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f59675d-5403-4a95-a5be-1dd613f2c82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "# Identificar 2 grupos de ocorrências mais baixas\n",
    "group_var = \"PaymentMethod\"\n",
    "stats_df = telco_no_outliers_df.groupBy(group_var) \\\n",
    "                      .agg(count(\"*\").alias(\"Total\"),\\\n",
    "                           avg(\"MonthlyCharges\").alias(\"MonthlyCharges\")) \\\n",
    "                      .orderBy(col(\"Total\").desc())\n",
    "\n",
    "# Exibir\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc9101a-b93e-4337-a6e7-2e8ff5471462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reúna os 2 grupos mais baixos supondo que o limiar de contagem é inferior a 20% do conjunto de dados completo e encargos mensais <20$\n",
    "N = telco_no_outliers_df.count()  # contagem total\n",
    "lower_groups = [elem[group_var] for elem in stats_df.head(2) if elem['Total']/N < 0.2 and elem['MonthlyCharges'] < 20]\n",
    "print(f\"Removendo grupos: {', '.join(lower_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcf8dd20-bb4d-4cf8-84f9-e96941e3756c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar/remover listagens destes grupos de baixa ocorrência mantendo ocorrências nulas\n",
    "telco_no_outliers_df = telco_no_outliers_df.filter( \\\n",
    "    ~col(group_var).isin(lower_groups) | \\\n",
    "    col(group_var).isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f6d2967-c90c-4a47-b9f1-b541f93eeca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar/Comparar conjuntos de dados antes/depois de remover outliers\n",
    "print(f\"Contagem - Antes: {telco_customer_casted_df.count()} / Depois: {telco_no_outliers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7aa69e2-40a0-4cc7-b1eb-0b7f2e9016ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize/Snap table [OPTIONAL/for instructor only]\n",
    "telco_no_outliers_df.write.mode(\"overwrite\").saveAsTable(telco_customer_full_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc13d19f-6901-46a5-a160-cae360f81dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Tratamento de Valores Ausentes\n",
    "\n",
    "Para lidar com valores ausentes no conjunto de dados, é necessário identificar colunas com altas porcentagens de dados ausentes e descartar essas colunas. Em seguida, ele remove linhas com valores ausentes. Colunas numéricas são preenchidas com 0, e colunas de string são preenchidas com 'N/A'. No geral, o código demonstra uma abordagem abrangente para lidar com valores ausentes no conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca98771a-4882-45b0-a5e3-f92d3c6377ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Eliminar Colunas\n",
    "\n",
    "* Crie um DataFrame chamado **`missing_df`** para contar os valores ausentes por coluna no conjunto de dados **`telco_no_outliers_df`**.\n",
    "\n",
    "* O DataFrame **`missing_df`** é então transposto para uma leitura mais fácil usando a função TransposeDF, o que permite uma análise mais fácil dos valores ausentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ebecd0d-d93e-462b-9ee0-cd491f600140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, concat_ws, collect_list # isnan\n",
    "\n",
    "\n",
    "def calculate_missing(input_df, show=True):\n",
    "  \"\"\"\n",
    "  Função auxiliar para calcular e exibir dados ausentes\n",
    "  \"\"\"\n",
    "\n",
    "  # Primeiro obter contagem de valores ausentes por coluna para obter um DF de linha singleton\n",
    "  missing_df_ = input_df.select([count(when(col(c).contains('None') | \\\n",
    "                                                  col(c).contains('NULL') | \\\n",
    "                                                  (col(c) == '' ) | \\\n",
    "                                                  col(c).isNull(), c)).alias(c) \\\n",
    "                                                  for c in input_df.columns\n",
    "                                            ])\n",
    "\n",
    "  # Transpor para melhor legibilidade\n",
    "  def TransposeDF(df, columns, pivotCol):\n",
    "    \"\"\"Função auxiliar para transpor dataframe do spark\"\"\"\n",
    "    columnsValue = list(map(lambda x: str(\"'\") + str(x) + str(\"',\")  + str(x), columns))\n",
    "    stackCols = ','.join(x for x in columnsValue)\n",
    "    df_1 = df.selectExpr(pivotCol, \"stack(\" + str(len(columns)) + \",\" + stackCols + \")\")\\\n",
    "            .select(pivotCol, \"col0\", \"col1\")\n",
    "    final_df = df_1.groupBy(col(\"col0\")).pivot(pivotCol).agg(concat_ws(\"\", collect_list(col(\"col1\"))))\\\n",
    "                  .withColumnRenamed(\"col0\", pivotCol)\n",
    "    return final_df\n",
    "\n",
    "  missing_df_out_T = TransposeDF(\n",
    "    spark.createDataFrame([{\"Column\":\"Número de Valores Ausentes\"}]).join(missing_df_),\n",
    "    missing_df_.columns,\n",
    "    \"Column\").withColumn(\"Número de Valores Ausentes\", col(\"Número de Valores Ausentes\").cast(\"long\"))\n",
    "\n",
    "  if show:\n",
    "    display(missing_df_out_T.orderBy(\"Número de Valores Ausentes\", ascending=False))\n",
    "\n",
    "  return missing_df_out_T\n",
    "\n",
    "missing_df = calculate_missing(telco_no_outliers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d13665-7404-44c5-aa74-f389aa9a1f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Remover colunas com mais de x% de linhas faltantes**\n",
    "\n",
    "Colunas com mais de 60% de dados faltantes são identificadas e armazenadas na lista **`to_drop_missing`**, e essas colunas são posteriormente removidas do conjunto de dados **`telco_no_outliers_df`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3fbbe6d-e198-413a-9518-752b494b218f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "per_thresh = 0.6 # Drop if column has more than 80% missing data\n",
    "\n",
    "N = telco_no_outliers_df.count() # total count\n",
    "to_drop_missing = [x.asDict()['Column'] for x in missing_df.select(\"Column\").where(col(\"Number of Missing Values\") / N >= per_thresh).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "919a6e1f-9df2-486c-be1f-f242438be919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Eliminando colunas {to_drop_missing} para mais de {per_thresh * 100}% de dados ausentes\")\n",
    "telco_no_missing_df = telco_no_outliers_df.drop(*to_drop_missing)\n",
    "# display(telco_no_missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ae5131-908d-45be-bc2a-d7873450a801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Remover linhas que contêm números específicos de colunas/campos ausentes**\n",
    "\n",
    "Linhas com mais de 1/4 das colunas com valores ausentes são descartadas usando **`na.drop()`** e os valores ausentes restantes nas colunas numéricas são imputados com 0, enquanto os valores ausentes nas colunas de string são imputados com 'N/A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b42446e-c326-4ccd-85ff-f3655a08b90c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_cols = len(telco_no_missing_df.columns)\n",
    "telco_no_missing_df = telco_no_missing_df.na.drop(how='any', thresh=round(n_cols/4)) # Elimine as linhas em que pelo menos metade dos valores estão faltando, how='all' também pode ser usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65e604ea-1ca3-4769-812c-ec00b2445951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar/Comparar conjuntos de dados antes/depois de remover ausentes\n",
    "print(f\"Contagem - Antes: {telco_no_outliers_df.count()} / Depois: {telco_no_missing_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4a9795-9345-4926-9f00-fd5ca834ce72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Imputar Dados Ausentes\n",
    "\n",
    "Substitua valores ausentes por um valor de substituição especificado.\n",
    "\n",
    "* As listas **`num_cols`** e **`string_cols`** são criadas para identificar colunas numéricas e de string no conjunto de dados, respectivamente.\n",
    "\n",
    "* Finalmente, valores ausentes nas colunas numéricas e de string são imputados com valores apropriados usando **`na.fill()`**, resultando no conjunto de dados **`telco_imputed_df`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a17fc2-2f71-4e17-b560-bb1c91544f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Substituir numérico faltante com constante/0**\n",
    "\n",
    "OBSERVAÇÃO: não se aplica ao caso deste conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24e3052-8730-4ff3-a61d-49205e3fa86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "\n",
    "# Obtenha uma lista de colunas numéricas\n",
    "num_cols = [c.name for c in telco_no_missing_df.schema.fields if (c.dataType == DoubleType() or c.dataType == IntegerType())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "856def7d-5761-431c-bca7-328749aef0dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imputar\n",
    "# telco_imputed_df = telco_no_missing_df.na.fill(value=0, subset=num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1909107e-631d-4f69-90f9-e5d7d7b8efaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Substitua booleano ausente com `False`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8676d653-ff92-436d-aaba-188c8948a21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "\n",
    "# Obtenha uma lista de colunas booleanas\n",
    "bool_cols = [c.name for c in telco_no_missing_df.schema.fields if (c.dataType == BooleanType())]\n",
    "\n",
    "# Imputar\n",
    "telco_imputed_df = telco_no_missing_df.na.fill(value=False, subset=bool_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b8c0d2-92ab-4e93-824e-da52618c7480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Substitua a string missing com `No`**\n",
    "\n",
    "Todas as colunas de string, exceto `gender`, `Contract` e `PaymentMethod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39feea8b-a224-455e-8753-723744cea581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Obter lista de colunas de string\n",
    "to_exclude = [\"customerID\", \"gender\", \"Contract\", \"PaymentMethod\"]\n",
    "string_cols = [c.name for c in telco_no_missing_df.drop(*to_exclude).schema.fields if c.dataType == StringType()]\n",
    "\n",
    "# Imputar\n",
    "telco_imputed_df = telco_imputed_df.na.fill(value='No', subset=string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc895b0c-bcb3-483c-9f36-69bd15f0aa71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comparar estatísticas ausentes novamente\n",
    "calculate_missing(telco_imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d25d084-0612-4d32-b853-3650c736f489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_imputed_df.write.mode(\"overwrite\").saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_imputed_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb8e09e-8e2f-440f-84e5-559dc7ddd0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Codificação de Recursos Categóricos\n",
    "\n",
    "Nesta seção, nós iremos codificar um-hot os recursos categóricos/string usando o avaliador `OneHotEncoder` da Spark MLlib.\n",
    "\n",
    "Se você não está familiarizado com a codificação one-hot, há uma descrição abaixo. Se você já está familiarizado, pode pular para a seção **Codificação one-hot no Spark MLlib** em direção ao fundo da célula.\n",
    "\n",
    "#### Recursos categóricos no aprendizado de máquina\n",
    "\n",
    "Muitos algoritmos de aprendizado de máquina não são capazes de aceitar recursos categóricos como entradas. Como resultado, cientistas de dados e engenheiros de aprendizado de máquina precisam determinar como lidar com eles.\n",
    "\n",
    "Uma solução fácil seria remover os recursos categóricos do conjunto de recursos. Embora isso seja rápido, **você está removendo informações potencialmente preditivas** &mdash; portanto, geralmente não é a melhor estratégia.\n",
    "\n",
    "Outras opções incluem maneiras de representar recursos categóricos como recursos numéricos. Algumas opções comuns são:\n",
    "\n",
    "1. **Codificação one-hot**: criar variáveis dummy/binárias para cada categoria\n",
    "2. **Codificação de rótulo/alvo**: substituir cada valor de categoria com um valor que representa a variável de destino (por exemplo, substituir um valor de categoria específico com a média da variável de destino para linhas com esse valor de categoria)\n",
    "3. **Embutimentos**: usar/criar uma representação vetorial de palavras significativas em cada valor de categoria\n",
    "\n",
    "Cada uma dessas opções pode ser realmente útil em diferentes cenários. Vamos nos concentrar na codificação one-hot aqui.\n",
    "\n",
    "#### Noções básicas de codificação one-hot\n",
    "\n",
    "A codificação one-hot cria um recurso binário/dummy para cada categoria em cada recurso categórico.\n",
    "\n",
    "No exemplo abaixo, o recurso **Animal** é dividido em três recursos binários &mdash; um para cada valor em **Animal**. O valor de cada recurso binário é igual a 1 se seu respectivo valor de categoria estiver presente em **Animal** para cada linha. Se seu valor de categoria não estiver presente na linha, o valor do recurso binário será 0.\n",
    "\n",
    "![Imagem de codificação one-hot](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/one-hot-encoding.png)\n",
    "\n",
    "#### Codificação one-hot no Spark MLlib\n",
    "\n",
    "Mesmo que você entenda a codificação one-hot, é importante aprender como executá-la usando o Spark MLlib.\n",
    "\n",
    "Para codificar one-hot recursos categóricos no Spark MLlib, usaremos duas classes: [a classe **`StringIndexer`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html#pyspark.ml.feature.StringIndexer) e [a classe **`OneHotEncoder`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html#pyspark.ml.feature.OneHotEncoder).\n",
    "\n",
    "* A classe `StringIndexer` indexa colunas do tipo string para um índice numérico. Cada valor exclusivo na coluna do tipo string é mapeado para um inteiro exclusivo.\n",
    "* A classe `OneHotEncoder` aceita colunas indexadas e as converte em um recurso do tipo vetor codificado em one-hot.\n",
    "\n",
    "#### Aplicando o fluxo de trabalho `StringIndexer` -> `OneHotEncoder` -> `VectorAssembler`\n",
    "\n",
    "Primeiro, precisaremos indexar os recursos categóricos do DataFrame. `StringIndexer` recebe alguns argumentos:\n",
    "\n",
    "1. Uma lista de colunas categóricas para indexar.\n",
    "2. Uma lista de nomes para as colunas indexadas que estão sendo criadas.\n",
    "3. Instruções sobre como lidar com novas categorias ao transformar dados.\n",
    "\n",
    "Como `StringIndexer` precisa aprender quais categorias estão presentes antes de indexar, é um **estimador** &mdash; lembre-se de que isso significa que precisamos chamar seu método `fit`. Seu resultado pode então ser usado para transformar nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e46bcd8-a534-48b8-8093-a15a77b6c123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df = telco_imputed_df.select(\"Contract\").distinct()\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6932fa6-6ab5-4acc-806c-7312935d613c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# StringIndexer\n",
    "string_cols = [\"Contract\"]\n",
    "index_cols = [column + \"_index\" for column in string_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=string_cols, outputCols=index_cols, handleInvalid=\"skip\")\n",
    "string_indexer_model = string_indexer.fit(sample_df)\n",
    "indexed_df = string_indexer_model.transform(sample_df)\n",
    "\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0431beb0-7c0b-4d85-9f58-9b9d44c0282d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uma vez que nossos dados tenham sido indexados, estamos prontos para usar o estimador `OneHotEncoder`.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"/>&nbsp;**Dica:** Veja a [documentação do `OneHotEncoder`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html#pyspark.ml.feature.OneHotEncoder) e nossos fluxos de trabalho anteriores do Spark MLlib que usam estimadores para obter orientação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09a4472-ca2e-406e-8095-a92fda97c944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "\n",
    "# Crie uma lista de nomes de recursos codificados em um-hot\n",
    "ohe_cols = [column + \"_ohe\" for column in string_cols]\n",
    "\n",
    "# Instancie o OneHotEncoder com as listas de colunas\n",
    "ohe = OneHotEncoder(inputCols=index_cols, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "# Ajuste o OneHotEncoder nos dados indexados\n",
    "ohe_model = ohe.fit(indexed_df)\n",
    "\n",
    "# Transforme indexed_df usando o ohe_model\n",
    "ohe_df = ohe_model.transform(indexed_df)\n",
    "ohe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b05a04-70fd-424c-b3d4-a3b6a1012406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "selected_ohe_cols = [\"Contract_ohe\"]\n",
    "\n",
    "# Use VectorAssembler para montar as colunas codificadas one-hot selecionadas em um vetor denso\n",
    "assembler = VectorAssembler(inputCols=selected_ohe_cols, outputCol=\"features\")\n",
    "result_df_dense = assembler.transform(ohe_df)\n",
    "\n",
    "# Selecione colunas relevantes para exibição\n",
    "result_df_display = result_df_dense.select(\"Contract\", \"features\")\n",
    "\n",
    "result_df_display.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a76b28a4-9f28-4180-984d-a221b049b556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Aplique incorporações pré-existentes a recursos categóricos/disseminados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4772120d-3501-473f-bb12-445f0065607c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vamos agrupar **`tenure`** para converter os dados discretos em formato de categorias/bins para uma análise e modelagem posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a314323d-da08-4230-bc16-d22b1adaaed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_to_bin = \"tenure\"\n",
    "display(telco_imputed_df.select(column_to_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37c0fb2a-0f7d-46c5-9925-c232a8ae9a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Especificar faixas de bin e coluna para binarizar\n",
    "bucketizer = Bucketizer(\n",
    "    splits=[0, 24, 48, float('Inf')],\n",
    "    inputCol=column_to_bin,\n",
    "    outputCol=f\"{column_to_bin}_bins\"\n",
    ")\n",
    "\n",
    "# Aplicar o bucketizer ao DataFrame\n",
    "bins_df = bucketizer.transform(telco_imputed_df.select(column_to_bin))\n",
    "\n",
    "# Reformule os números bin para inteiros\n",
    "bins_df = bins_df.withColumn(f\"{column_to_bin}_bins\", col(f\"{column_to_bin}_bins\").cast(\"integer\"))\n",
    "\n",
    "# Mostrar o resultado\n",
    "display(bins_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "769b944a-188a-4765-809d-4a3bd04d8520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mapear de volta para os escores de embedding legíveis por humanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49837a78-7605-4cfd-b081-1a40d2a693a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bins_embedded_df = (\n",
    "  bins_df.withColumn(f\"{column_to_bin}_embedded\", col(f\"{column_to_bin}_bins\").cast(StringType()))\n",
    "         .replace(to_replace = \n",
    "                  {\n",
    "                    \"0\":\"<2y\",\n",
    "                    \"1\":\"2-4y\",\n",
    "                    \"2\":\">4y\"\n",
    "                  },\n",
    "                  subset=[f\"{column_to_bin}_embedded\"])\n",
    ")\n",
    "display(bins_embedded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9826366b-87c0-4e82-a3f1-171056ee4a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Indexação Ordenada\n",
    "\n",
    "Realize indexação ordenada como uma preparação alternativa de recursos categóricos para modelagem de floresta aleatória.\n",
    "\n",
    "Algumas categorias são, na verdade, `ordinais` e, portanto, podem exigir codificação adicional/manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba83b7a-322b-4a9e-8878-60470b0411ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ordinal_cat = \"Contract\"\n",
    "telco_imputed_df.select(ordinal_cat).distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7d7da9-e7ec-4054-925a-8feabf32e50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir mapa/dicionário de Ordinal (categoria:índice)\n",
    "ordered_list = [\n",
    "    \"Month-to-month\",\n",
    "    \"One year\",\n",
    "    \"Two year\"\n",
    "]\n",
    "\n",
    "ordinal_dict = {category: f\"{index+1}\" for index, category in enumerate(ordered_list)}\n",
    "display(ordinal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252087e1-6ad2-473f-86e2-7c96ee2e09f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crie uma nova coluna com indexação ordenada\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "ordinal_df = (\n",
    "    telco_imputed_df\n",
    "   .withColumn(f\"{ordinal_cat}_ord\", col(ordinal_cat)) # Duplicar\n",
    "   .replace(to_replace=ordinal_dict, subset=[f\"{ordinal_cat}_ord\"]) # Mapear\n",
    "   .withColumn(f\"{ordinal_cat}_ord\", col(f\"{ordinal_cat}_ord\").cast('int')) # Converter para inteiro\n",
    ")\n",
    "\n",
    "display(ordinal_df.select(ordinal_cat, f\"{ordinal_cat}_ord\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e595a0f1-83dd-46a7-897b-290ed1f58b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Dividindo Dados (para Validação Cruzada)\n",
    "\n",
    "Divida os dados de modelagem em um conjunto de treino, teste e validação como parte de um processo de modelagem\n",
    "\n",
    "Nesta seção, realizaremos o fluxo de trabalho recomendado para uma divisão treino-teste usando a API DataFrame do Spark.\n",
    "\n",
    "Lembre-se de que, devido a fatores como configurações de cluster em constante mudança e particionamento de dados, pode ser difícil garantir uma divisão treino-teste reproduzível. Como resultado, recomendamos:\n",
    "\n",
    "1. Divida os dados usando a **mesma semente aleatória**\n",
    "2. Escreva os DataFrames de treino e teste\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"/>&nbsp;**Dica:** Confira a [**documentação `randomSplit`**](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cfb252b-fe9e-44af-87d5-9ea0952b7826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dividir com 80 por cento dos dados em train_df e 20 por cento dos dados em test_df\n",
    "train_df, test_df = telco_imputed_df.randomSplit([.8,.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fa8eff-dfa3-4826-a0bd-951bf63c474c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize (OPCIONAL)\n",
    "train_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_customers_train\")\n",
    "test_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_customers_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "745fbba9-9fd4-4f7b-8ad4-61ee23d56086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Padronizar Recursos em um Conjunto de Treinamento\n",
    "\n",
    "Para fins de exemplo, escolheremos uma coluna sem dados ausentes (por exemplo, `MonthlyCharges`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f5c5fd-f177-4164-8454-a6fd192076c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, RobustScaler, VectorAssembler\n",
    "\n",
    "\n",
    "num_cols_to_scale = [\"MonthlyCharges\"] # colunas numéricas\n",
    "assembler = VectorAssembler().setInputCols(num_cols_to_scale).setOutputCol(\"numerical_assembled\")\n",
    "train_assembled_df = assembler.transform(train_df.select(*num_cols_to_scale))\n",
    "test_assembled_df = assembler.transform(test_df.select(*num_cols_to_scale))\n",
    "\n",
    "# Define o escalonador e ajusta-o no conjunto de treinamento\n",
    "scaler = RobustScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "scaler_fitted = scaler.fit(train_assembled_df)\n",
    "\n",
    "\n",
    "# Aplica-se a ambos os conjuntos de treinamento e teste\n",
    "train_scaled_df = scaler_fitted.transform(train_assembled_df)\n",
    "test_scaled_df = scaler_fitted.transform(test_assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b990dd6-747d-47cc-8f94-3014892c45ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Peek at Training set\")\n",
    "train_scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "588a3f34-a076-4f5c-aa09-8ffb70d690f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Peek at Test set\")\n",
    "test_scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b177954-7a7f-4d16-bccd-dc365f3b991b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imputar valores categoricos ausentes com o valor de moda usando sparkml\n",
    "\n",
    "Como lidar com dados ausentes apenas no momento do treinamento e incorporá-los como parte do pipeline de inferência para evitar vazamentos de dados e garantir que a observação com dados ausentes seja usada para treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fefb22f2-007b-4c80-97d0-84fdc46b6c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols_to_impute = [\"PaymentMethod\"] # string_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91caca0f-f7a9-4294-8595-c2daf355131f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Indexar categoricals primeiro, pois `Imputer` não lida com categoricals diretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b2e9aa2-2012-4c1c-ae7d-2ca237257eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Indexar colunas categóricas usando StringIndexer\n",
    "cat_index_cols = [column + \"_index\" for column in categorical_cols_to_impute]\n",
    "cat_indexer = StringIndexer(\n",
    "    inputCols=categorical_cols_to_impute,\n",
    "    outputCols=cat_index_cols,\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Treinar no conjunto de treinamento\n",
    "cat_indexer_model = cat_indexer.fit(train_df.select(categorical_cols_to_impute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3d2f0e-b7ba-4847-ba13-7937639e74a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformar ambos os conjuntos de treino e teste usando o modelo ajustado StringIndexer\n",
    "cat_indexed_train_df = cat_indexer_model.transform(train_df.select(*categorical_cols_to_impute))\n",
    "cat_indexed_test_df = cat_indexer_model.transform(test_df.select(*categorical_cols_to_impute))\n",
    "# display(cat_indexed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a81d5b-8c1f-4e4e-83ed-c351c21e09de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676a393c-75e2-47c6-8d24-323ccc777820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O `StringIndexer` criará um novo rótulo (_por exemplo_, `4`) para ausente ao definir a flag `handleInvalid` como `keep`, por isso é importante manter o controle/reverter os valores de índice de volta para `null` se quisermos imputá-los, caso contrário `null` será tratado como sua própria/categoria separada automaticamente.\n",
    "\n",
    "Como alternativa para imputar categorias/strings, podemos usar o método `.fillna()` fornecendo o valor `mode` manualmente (conforme descrito acima)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70973e0-1ed3-46a0-b378-7bbcf0afe681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revert indexes to `null` for missing categories\n",
    "for c in categorical_cols_to_impute:\n",
    "    cat_indexed_train_df = cat_indexed_train_df.withColumn(f\"{c}_index\", when(col(c).isNull(), None).otherwise(col(f\"{c}_index\")))\n",
    "    cat_indexed_test_df = cat_indexed_test_df.withColumn(f\"{c}_index\", when(col(c).isNull(), None).otherwise(col(f\"{c}_index\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda3e310-c749-49ef-a7b9-c86262ed74d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "144d1da8-39cd-4d0e-86a7-a17735829ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ajuste o imputer nos categoricals indexados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c0bb12d-2c78-4123-baf5-3c0e114b5918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\n",
    "# Definir modo de imputação\n",
    "output_cat_index_cols_imputed = [col+'_imputed' for col in cat_index_cols]\n",
    "mode_imputer = Imputer(\n",
    "  inputCols=cat_index_cols,\n",
    "  outputCols=output_cat_index_cols_imputed,\n",
    "  strategy=\"mode\"\n",
    "  )\n",
    "\n",
    "# Ajustar em training_df\n",
    "mode_imputer_fitted = mode_imputer.fit(cat_indexed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6afcda8a-f803-473c-b176-8ddb71eb63a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transforme conjuntos de treinamento e teste\n",
    "cat_indexed_train_imputed_df = mode_imputer_fitted.transform(cat_indexed_train_df)\n",
    "cat_indexed_test_imputed_df  = mode_imputer_fitted.transform(cat_indexed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c72a0eb-0e5a-4163-b802-aeec3935e7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vislumbre o conjunto de testes\n",
    "display(cat_indexed_test_imputed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e54b7e1-f0e0-48ad-89ef-04aac4e8a946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Limpar Sala de Aula\n",
    "\n",
    "Execute a célula a seguir para remover os ativos específicos de lições criados durante esta lição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddf19496-e5d4-45e3-bcdf-8ea674ee2ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb9e7965-e95f-4973-908d-757c8665538a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusão\n",
    "\n",
    "Esta demonstração forneceu uma compreensão abrangente da preparação de dados para modelagem e preparação de recursos, equipando-o com o conhecimento e habilidades para preparar seus dados de forma eficaz para modelagem e análise. Vimos de maneira contínua como corrigir o tipo de dados, identificar e remover outliers, lidar com valores ausentes por meio de imputação ou substituição, codificar recursos categóricos e padronizar recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c2cb85-02dd-428e-a979-8dceda14c27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.1 - Data Imputation and Transformation Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
