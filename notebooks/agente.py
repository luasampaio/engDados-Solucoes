"""
Exemplo Completo Databricks: Validação de Qualidade de Dados
Integração com Unity Catalog e Delta Live Tables

Este notebook demonstra:
1. Setup completo do ambiente
2. Criação de dados de exemplo com problemas de qualidade
3. Validação usando o agente LLM
4. Armazenamento de regras e resultados no Unity Catalog
5. Integração com Delta Live Tables
6. Agendamento e monitoramento
7. Alertas automatizados

Para executar: Upload para Databricks Workspace e execute como notebook
"""

# Databricks notebook source
# MAGIC %md
# MAGIC # Agente LLM para Validação de Qualidade de Dados - Exemplo Completo
# MAGIC 
# MAGIC **Objetivo:** Demonstrar integração completa com Unity Catalog e Delta Live Tables
# MAGIC 
# MAGIC **Pré-requisitos:**
# MAGIC - Cluster Databricks com DBR 13.3+
# MAGIC - Unity Catalog habilitado
# MAGIC - OpenAI API Key configurada como secret
# MAGIC - Permissões para criar tabelas em `main.data_quality`

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup e Instalação

# COMMAND ----------

# Instalar dependências
%pip install openai==1.12.0 pydantic==2.6.0 --quiet

# COMMAND ----------

# Reiniciar Python para carregar novos pacotes
dbutils.library.restartPython()

# COMMAND ----------

# Importar bibliotecas necessárias
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime, timedelta
import random
import json
import os

# Configurar OpenAI API Key do Databricks Secrets
os.environ["OPENAI_API_KEY"] = dbutils.secrets.get(scope="llm-keys", key="openai-api-key")

print("✓ Bibliotecas importadas com sucesso")
print(f"✓ Spark version: {spark.version}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Preparar Estrutura no Unity Catalog

# COMMAND ----------

# Criar catalog e schema para data quality (se não existir)
spark.sql("""
CREATE CATALOG IF NOT EXISTS main
COMMENT 'Main production catalog'
""")

spark.sql("""
CREATE SCHEMA IF NOT EXISTS main.data_quality
COMMENT 'Schema for data quality validation artifacts'
""")

# Criar tabelas para armazenar regras e resultados
spark.sql("""
CREATE TABLE IF NOT EXISTS main.data_quality.validation_rules (
    rule_id STRING,
    table_name STRING,
    rule_name STRING,
    rule_description STRING,
    column_name STRING,
    rule_type STRING,
    severity STRING,
    parameters STRING,
    created_at TIMESTAMP,
    created_by STRING
)
USING DELTA
COMMENT 'Validation rules generated by LLM agent'
""")

spark.sql("""
CREATE TABLE IF NOT EXISTS main.data_quality.validation_reports (
    execution_id STRING,
    dataset_name STRING,
    execution_timestamp TIMESTAMP,
    total_rules INT,
    passed_rules INT,
    failed_rules INT,
    error_rules INT,
    quality_score DOUBLE,
    execution_time_seconds DOUBLE,
    decision STRING,
    report_json STRING
)
USING DELTA
COMMENT 'Data quality validation execution reports'
""")

spark.sql("""
CREATE TABLE IF NOT EXISTS main.data_quality.validation_results (
    execution_id STRING,
    rule_id STRING,
    rule_name STRING,
    column_name STRING,
    status STRING,
    violations_count BIGINT,
    total_records BIGINT,
    violation_percentage DOUBLE,
    execution_time_seconds DOUBLE,
    error_message STRING,
    execution_timestamp TIMESTAMP
)
USING DELTA
COMMENT 'Detailed validation results per rule'
""")

print("✓ Unity Catalog estrutura criada:")
print("  - main.data_quality.validation_rules")
print("  - main.data_quality.validation_reports")
print("  - main.data_quality.validation_results")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Criar Dataset de Exemplo com Problemas de Qualidade

# COMMAND ----------

# Definir schema do dataset
schema = StructType([
    StructField("order_id", StringType(), True),
    StructField("order_item_id", IntegerType(), True),
    StructField("product_id", StringType(), True),
    StructField("seller_id", StringType(), True),
    StructField("shipping_limit_date", TimestampType(), True),
    StructField("price", DoubleType(), True),
    StructField("freight_value", DoubleType(), True),
    StructField("customer_name", StringType(), True),
    StructField("customer_email", StringType(), True),
    StructField("customer_cpf", StringType(), True),
    StructField("order_status", StringType(), True)
])

# Função para gerar dados com problemas de qualidade intencionais
def generate_sample_data(num_rows=5000):
    """
    Gera dados de exemplo com problemas de qualidade conhecidos:
    - 2% de order_id nulos
    - 3% de emails com formato inválido
    - 1% de preços negativos
    - 5% de customer_name nulos
    - 10% de CPFs inválidos
    - Duplicatas de order_id (0.5%)
    """
    data = []
    base_date = datetime(2024, 1, 1)
    used_order_ids = set()
    
    statuses = ["completed", "pending", "cancelled", "processing", "shipped"]
    
    for i in range(num_rows):
        # Gerar order_id com possibilidade de nulo e duplicatas
        if i % 50 == 0:  # 2% nulos
            order_id = None
        elif i % 200 == 0 and len(used_order_ids) > 0:  # 0.5% duplicatas
            order_id = random.choice(list(used_order_ids))
        else:
            order_id = f"ORD{i:08d}"
            used_order_ids.add(order_id)
        
        # Email com formato inválido em 3% dos casos
        if i % 33 == 0:
            customer_email = f"invalid_email_{i}"
        else:
            customer_email = f"customer{i}@email.com.br"
        
        # Preço negativo em 1% dos casos
        if i % 100 == 0:
            price = round(random.uniform(-100, -10), 2)
        else:
            price = round(random.uniform(10, 2000), 2)
        
        # Customer name nulo em 5% dos casos
        customer_name = None if i % 20 == 0 else f"Cliente {i}"
        
        # CPF inválido em 10% dos casos
        if i % 10 == 0:
            customer_cpf = f"{random.randint(0, 999999999):09d}"  # CPF sem formatação
        else:
            customer_cpf = f"{random.randint(100, 999)}.{random.randint(100, 999)}.{random.randint(100, 999)}-{random.randint(10, 99)}"
        
        row = (
            order_id,
            random.randint(1, 5),
            f"PROD{random.randint(1, 500):05d}",
            f"SELLER{random.randint(1, 100):04d}",
            base_date + timedelta(days=random.randint(0, 365), hours=random.randint(0, 23)),
            price,
            round(random.uniform(5, 100), 2),
            customer_name,
            customer_email,
            customer_cpf,
            random.choice(statuses)
        )
        data.append(row)
    
    return data

# Gerar dados
print("Gerando dataset de exemplo com problemas de qualidade...")
sample_data = generate_sample_data(5000)
df_bronze = spark.createDataFrame(sample_data, schema)

# Salvar em bronze layer
df_bronze.write.format("delta").mode("overwrite").saveAsTable("main.data_quality.bronze_orders")

print(f"✓ Dataset criado: {df_bronze.count()} registros")
print(f"✓ Salvo em: main.data_quality.bronze_orders")
print("\nAmostra dos dados:")
display(df_bronze.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Carregar Código do Agente

# COMMAND ----------

# Importar módulos do agente
# Assumindo que os arquivos foram carregados em /Workspace/data_quality_agent/src
import sys
sys.path.append('/Workspace/data_quality_agent/src')

from models import AgentConfig, ValidationRule, DataQualityReport
from profiler import SparkDataProfiler
from llm_agent import DataQualityLLMAgent
from validator import SparkValidationExecutor
import uuid
import time

print("✓ Módulos do agente importados com sucesso")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Implementar Agente Databricks Completo

# COMMAND ----------

class DatabricksDataQualityAgent:
    """
    Agente completo para Databricks com integração Unity Catalog
    """
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.profiler = SparkDataProfiler(
            sample_size=config.sample_size,
            max_distinct_values=config.max_distinct_values
        )
        self.llm_agent = DataQualityLLMAgent(config)
        self.validator = SparkValidationExecutor(
            max_workers=config.max_workers,
            timeout_seconds=config.timeout_seconds
        )
        self.generated_rules = []
        self.generated_sql = []
    
    def validate_spark_dataframe(self, spark_df, table_name: str):
        """Executa validação completa com armazenamento no Unity Catalog"""
        
        execution_id = str(uuid.uuid4())
        start_time = time.time()
        
        print(f"\n{'='*70}")
        print(f"VALIDAÇÃO DE QUALIDADE DE DADOS")
        print(f"{'='*70}")
        print(f"Execution ID: {execution_id}")
        print(f"Dataset: {table_name}")
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")
        
        # 1. Profiling
        print("[1/6] Profiling do dataset...")
        profile = self.profiler.profile_spark_dataframe(spark_df, table_name)
        print(f"      ✓ {profile.total_rows} linhas, {profile.total_columns} colunas analisadas")
        
        # 2. Geração de regras
        print("[2/6] Gerando regras de validação via LLM...")
        self.generated_rules = self.llm_agent.generate_validation_rules(profile)
        print(f"      ✓ {len(self.generated_rules)} regras geradas")
        
        # 3. Armazenar regras no Unity Catalog
        if self.config.store_rules_in_catalog:
            print("[3/6] Armazenando regras no Unity Catalog...")
            self._store_rules_in_catalog(table_name, self.generated_rules)
            print(f"      ✓ Regras salvas em main.data_quality.validation_rules")
        else:
            print("[3/6] Armazenamento de regras desabilitado (pulando...)")
        
        # 4. Geração de SQL
        print("[4/6] Gerando queries SQL (Spark SQL)...")
        self.generated_sql = self.llm_agent.generate_sql_queries(
            self.generated_rules,
            table_name,
            dialect="spark"
        )
        print(f"      ✓ {len(self.generated_sql)} queries geradas")
        
        # 5. Executar validações
        print("[5/6] Executando validações...")
        report = self._execute_validations_custom(spark_df, table_name, execution_id)
        print(f"      ✓ Validações concluídas em {report.execution_time_seconds:.2f}s")
        
        # 6. Armazenar resultados
        print("[6/6] Armazenando resultados no Unity Catalog...")
        self._store_report_in_catalog(report)
        print(f"      ✓ Resultados salvos em main.data_quality.validation_reports")
        
        # Imprimir resumo
        self._print_summary(report)
        
        return report
    
    def _execute_validations_custom(self, spark_df, table_name, execution_id):
        """Executa validações e retorna relatório"""
        start_time = time.time()
        
        # Registrar como temp view
        spark_df.createOrReplaceTempView("validation_table")
        
        results = []
        
        for query_info in self.generated_sql:
            try:
                sql_query = query_info.get("sql_query", "")
                sql_query = sql_query.replace(table_name, "validation_table")
                
                # Executar query
                result_df = spark.sql(sql_query)
                result_row = result_df.collect()[0]
                
                # Parsear resultado
                status = result_row.asDict().get("status", "ERROR")
                violations = result_row.asDict().get("violations", 0)
                total = spark_df.count()
                
                result = {
                    "rule_id": query_info.get("rule_id", "unknown"),
                    "rule_name": query_info.get("rule_name", "Unknown"),
                    "column_name": self._extract_column_from_rule_id(query_info.get("rule_id", "")),
                    "status": status,
                    "violations_count": int(violations) if violations else 0,
                    "total_records": total,
                    "violation_percentage": (violations / total * 100) if total > 0 else 0.0,
                    "execution_time_seconds": 0.0,
                    "error_message": None
                }
                results.append(result)
                
            except Exception as e:
                result = {
                    "rule_id": query_info.get("rule_id", "unknown"),
                    "rule_name": query_info.get("rule_name", "Unknown"),
                    "column_name": "unknown",
                    "status": "ERROR",
                    "violations_count": 0,
                    "total_records": 0,
                    "violation_percentage": 0.0,
                    "execution_time_seconds": 0.0,
                    "error_message": str(e)
                }
                results.append(result)
        
        # Calcular métricas
        total_rules = len(results)
        passed_rules = sum(1 for r in results if r["status"] == "PASS")
        failed_rules = sum(1 for r in results if r["status"] == "FAIL")
        error_rules = sum(1 for r in results if r["status"] == "ERROR")
        quality_score = (passed_rules / total_rules * 100) if total_rules > 0 else 0.0
        
        execution_time = time.time() - start_time
        
        # Criar relatório
        report = {
            "execution_id": execution_id,
            "dataset_name": table_name,
            "execution_timestamp": datetime.now(),
            "total_rules": total_rules,
            "passed_rules": passed_rules,
            "failed_rules": failed_rules,
            "error_rules": error_rules,
            "overall_quality_score": quality_score,
            "execution_time_seconds": execution_time,
            "results": results
        }
        
        return type('Report', (), report)()
    
    def _extract_column_from_rule_id(self, rule_id):
        """Extrai nome da coluna do rule_id"""
        parts = rule_id.split("_")
        return parts[1] if len(parts) > 1 else "unknown"
    
    def _store_rules_in_catalog(self, table_name, rules):
        """Armazena regras no Unity Catalog"""
        rules_data = []
        current_user = spark.sql("SELECT current_user() as user").collect()[0]["user"]
        
        for rule in rules:
            rules_data.append({
                "rule_id": rule.rule_id,
                "table_name": table_name,
                "rule_name": rule.rule_name,
                "rule_description": rule.rule_description,
                "column_name": rule.column_name,
                "rule_type": rule.rule_type,
                "severity": rule.severity,
                "parameters": json.dumps({
                    "regex_pattern": rule.regex_pattern,
                    "min_value": rule.min_value,
                    "max_value": rule.max_value,
                    "expected_data_type": rule.expected_data_type
                }),
                "created_at": datetime.now(),
                "created_by": current_user
            })
        
        rules_df = spark.createDataFrame(rules_data)
        rules_df.write.format("delta").mode("append").saveAsTable("main.data_quality.validation_rules")
    
    def _store_report_in_catalog(self, report):
        """Armazena relatório no Unity Catalog"""
        # Determinar decisão baseada no score
        if report.overall_quality_score >= 95:
            decision = "APPROVED"
        elif report.overall_quality_score >= 80:
            decision = "WARNING"
        else:
            decision = "REJECTED"
        
        # Salvar relatório consolidado
        report_data = [{
            "execution_id": report.execution_id,
            "dataset_name": report.dataset_name,
            "execution_timestamp": report.execution_timestamp,
            "total_rules": report.total_rules,
            "passed_rules": report.passed_rules,
            "failed_rules": report.failed_rules,
            "error_rules": report.error_rules,
            "quality_score": report.overall_quality_score,
            "execution_time_seconds": report.execution_time_seconds,
            "decision": decision,
            "report_json": json.dumps({
                "results": [r for r in report.results]
            }, default=str)
        }]
        
        report_df = spark.createDataFrame(rep
(Content truncated due to size limit. Use page ranges or line ranges to read remaining content)


ao vivo

Pular para ao vivo
